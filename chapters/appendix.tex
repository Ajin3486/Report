\chapter{Detailed Experimental Settings}
\label{appendix:experimental_settings}

This appendix provides the specific hyperparameters used for the PPO training and the exact domain randomization ranges, ensuring reproducibility of the results presented in this work.

\section{PPO Hyperparameters}
Table \ref{tab:ppo_params} lists the hyperparameters used for the Proximal Policy Optimization (PPO) algorithm in Isaac Lab. These values were selected based on standard baselines for continuous control tasks.

\begin{table}[h]
\centering
\caption{PPO Hyperparameters}
\label{tab:ppo_params}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Total Timesteps & 19,660,800 \\
Number of Environments & 4096 \\
Horizon (Steps per Episode) & 32 \\
Batch Size & 131,072 \\
Mini-batch Size & 32,768 \\
Number of Mini-batches & 4 \\
Number of Epochs & 5 \\
Clip Range ($\epsilon$) & 0.1 \\
Entropy Coefficient & $1 \times 10^{-4}$ \\
Value Function Coefficient & 1.0 \\
Learning Rate & $1 \times 10^{-4}$ (Adaptive) \\
Discount Factor ($\gamma$) & 0.99 \\
GAE ($\lambda$) & 0.95 \\
Max Iterations & 150 \\
\bottomrule
\end{tabular}
\end{table}

\section{Reward Function Formulation}
The total reward $r_t$ at timestep $t$ is computed as:

\begin{equation}
r_t = r_{\text{track}} + r_{\text{smooth}} + r_{\text{bonus}}
\end{equation}

Where:
\begin{itemize}
    \item \textbf{Tracking Reward:} $r_{\text{track}} = 5.0 \times \exp(-\|e_{\text{pos}}\|^2 / 0.02)$
    \item \textbf{Guidance Reward:} $r_{\text{guide}} = 1.0 \times \exp(-\|e_q\|^2 / 0.05)$
    \item \textbf{Action Penalty:} $p_{\text{action}} = 0.01 \times \|a\|^2$
    \item \textbf{Smoothness Penalty:} $p_{\text{smooth}} = 0.1 \times \|\Delta a\|^2$
\end{itemize}

Total reward: $r = r_{\text{track}} + r_{\text{guide}} - p_{\text{action}} - p_{\text{smooth}}$


