\section{Validation in Isaac Lab }
To assess the performance of our residual reinforcement learning approach, we first evaluated the learned policy directly in the Isaac Lab 
training environment. This controlled simulation setting allows us to precisely measure the improvement brought by the AI agent compared to
 a Open-Loop controller under localized perturbations.

\subsection{Experimental Control Modes: Open-Loop vs. AI Closed-Loop}
Before presenting the quantitative results, we distinguish the two control strategies evaluated in this validation phase:

\begin{itemize}
    \item \textbf{Open-Loop (Baseline):} The robot executes the trajectory theoretically planned by MoveIt. In a perfect simulation, this would be accurate. However, in our ``Sim-to-Real'' validation scenarios, we introduce calibration errors (offset between the camera and the robot base) and sensor noise. In open-loop, the robot blindly follows the plan without correcting for these discrepancies, leading to significant end-effector errors.
    
    \item \textbf{Closed-Loop RL (AI):} This is the proposed residual learning approach. The AI agent acts as a high-frequency feedback controller (60Hz). It continuously monitors the \textit{observed} state (proprioception + noisy vision) and the tracking error. Unlike the baseline, the AI ``closes the loop'' by injecting real-time joint corrections ($\Delta q$) to minimize the distance to the perceived target. This allows it to dynamically compensate for the static calibration errors and filter out high-frequency sensor noise.
\end{itemize}

The achieved success rate exceeds 95\% on a test set comprising random trajectories with perception noise and simulated calibration error. This validates that the agent has successfully learned to correct the trajectories from the MoveIt planner.

\begin{table}[htbp]
\centering
\caption{Comparison of performance metrics between Open-Loop and Closed-Loop RL approaches.}
\label{tab:performance}

\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Open-Loop} & \textbf{Closed-Loop RL} & \textbf{Improvement} \\
\midrule
Mean error (mm) & 28.37 & 8.58 & \textbf{69.7\%} \\
Max error (mm) & 52.03 & 23.08 & \textbf{55.6\%} \\
Std error (mm) & 15.36 & 4.39 & \textbf{71.4\%} \\
\midrule
Avg speed (mm/s) & -- & 187.9 & -- \\
Max speed (mm/s) & -- & 587.5 & -- \\
\bottomrule
\end{tabular}

\end{table}

Table~\ref{tab:performance} presents quantitative metrics comparing open-loop and closed-loop RL approaches:
\begin{itemize}
    \item \textbf{Mean error}: 28.37mm $\rightarrow$ 8.58mm (69.7\% improvement)
    \item \textbf{Max error}: 52.03mm $\rightarrow$ 23.08mm (55.6\% improvement)
    \item \textbf{Standard deviation}: 15.36mm $\rightarrow$ 4.39mm (71.4\% improvement)
    \item \textbf{Speed}: Average 187.9 mm/s, peak 587.5 mm/s
\end{itemize}
All error metrics show $>$50\% improvement, confirming the effectiveness of the RL correction approach.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{images/fig1_error_comparison.png}
    \caption{Tracking accuracy comparison: Open-Loop vs Closed-Loop RL. The red curve shows the open-loop position error (mean=28.4mm, max=52.0mm), while the green curve demonstrates the closed-loop RL correction (mean=8.6mm, max=23.1mm), representing a 70\% improvement.}
    \label{fig:error_comparison}
\end{figure}

Figure~\ref{fig:error_comparison} shows the position error norm over time. The red curve (open-loop) exhibits mean error of 28.4mm with peaks exceeding 52mm, particularly during rapid direction changes at t=1.0s and t=3.8s. The green curve (closed-loop RL) maintains mean error of 8.6mm with maximum error below 23mm, representing a 70\% improvement. The RL agent anticipates errors and attenuates disturbances without introducing oscillations.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{images/fig2_ee_position_x.png}
    \caption{Trajectory tracking on X-axis. The blue dashed line represents the desired trajectory, the orange dotted line shows the observed trajectory with noise, and the green solid line indicates the actual trajectory executed by the RL agent.}
    \label{fig:trajectory_x}
\end{figure}

Figure~\ref{fig:trajectory_x} illustrates end-effector tracking along the X-axis over a complex trajectory with multiple phases. The blue line represents the desired reference trajectory from MoveIt, exhibiting three distinct motion segments: a smooth ascent (0--2 s, 800 mm$\rightarrow$945 mm), an abrupt return to origin ($t\approx 2$ s), a second ascent phase (2--4 s), and a final rapid descent ($t\approx 4$ s). The red curve shows the observed trajectory corrupted by sensor noise ($\pm$2 mm Gaussian jitter) and systematic calibration error ($\pm$20 mm bias), creating high-frequency oscillations around the true position. 
Figure~\ref{fig:trajectory_x} illustrates end-effector tracking along the X-axis over a complex trajectory with multiple phases. The blue line represents the desired reference trajectory from MoveIt, exhibiting three distinct motion segments: a smooth ascent (0--2s, 800mm$\rightarrow$945mm), an abrupt return to origin ($t\approx2$s), a second ascent phase (2--4s), and a final rapid descent ($t\approx4$s). The red curve shows the observed trajectory corrupted by sensor noise ($\pm$2mm Gaussian jitter) and systematic calibration error ($\pm$20mm bias), creating high-frequency oscillations around the true position. 

The green curve demonstrates the RL agent's actual executed trajectory. Despite receiving noisy observations, the agent maintains tight tracking of the desired reference with 5.3mm mean error. Critically, the agent does not blindly follow the noisy measurements but instead acts as an intelligent filter: it extracts the underlying signal from corrupted observations while maintaining smooth motion. Even during rapid direction changes at t=2s and t=4s where noise peaks occur, the RL policy remains stable without overshoot or oscillation, demonstrating robust closed-loop control that compensates for both perception errors and dynamic disturbances.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{images/fig4_joint_j4_strategy.png}
    \caption{Joint J4 (wrist\_1) position comparison. Top: open-loop executes MoveIt reference perfectly but suffers from calibration errors at task-space level. Bottom: closed-loop RL intentionally deviates by +0.63° to compensate for systematic errors.}
    \label{fig:joint_j4}
\end{figure}

Figure~\ref{fig:joint_j4} reveals the fundamental difference between open-loop and closed-loop control at the joint level for J4 (wrist\_1). \textbf{Top panel (Open-Loop):} 
The robot perfectly tracks the MoveIt reference trajectory (blue curve matches executed joint positions). There is \textit{no joint-level tracking error} because the controller faithfully executes
 the planned joint commands. However, this reference was computed using an erroneous camera calibration — therefore, despite perfect joint tracking, 
 the end-effector misses its Cartesian target by 28mm on average.

\textbf{Bottom panel (Closed-Loop RL):} The RL agent intentionally deviates from the reference by applying a systematic +0.63\textdegree\ bias on J4. This is not tracking error but a \textit{deliberate correction}. The gap between the blue reference and the green executed trajectory shows that the agent has learned to compensate for the calibration error by exploiting kinematic redundancy. By slightly adjusting this wrist joint, the policy corrects the end-effector position without requiring recalibration or replanning. This demonstrates intelligent closed-loop compensation: the agent ``knows'' the reference is biased and proactively corrects it to achieve the true task-space objective.
\textbf{Bottom panel (Closed-Loop RL):} The RL agent intentionally deviates from the reference by applying a systematic +0.63\textdegree\ bias on J4. 
This is not tracking error but a \textit{deliberate correction}. The gap between the blue reference and the orange executed trajectory shows that the agent has learned to compensate
 for the calibration error by exploiting kinematic redundancy. By slightly adjusting this wrist joint, the policy corrects the end-effector position without requiring recalibration or replanning.
  This demonstrates intelligent closed-loop compensation: the agent "knows" the reference is biased and proactively corrects it to achieve the true task-space objective.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{images/fig3_ee_speed.png}
    \caption{End-effector speed during trajectory execution. Average speed: 187.9 mm/s, maximum speed: 587.5 mm/s.}
    \label{fig:ee_speed}
\end{figure}

Figure~\ref{fig:ee_speed} shows the end-effector velocity profile with average speed of 187.9 mm/s and peak of 587.5 mm/s at t=2.0s. 
The smooth profile without high-frequency oscillations indicates stable control. The velocity modulation demonstrates an emergent speed-accuracy trade-off learned during training,
 where the agent adaptively adjusts speed based on trajectory requirements while maintaining safety limits. This is crucial in our context: during trajectory segments with high uncertainty (noisy perception or larger calibration errors), the agent automatically slows down to improve tracking precision, while during low-error phases it increases speed for efficiency. This adaptive behavior is not explicitly programmed but emerges from the RL training objective, showing that the policy has learned an intelligent balance between speed and accuracy—a key requirement for reliable real-world deployment.

\section{Limitations of this validation}
Although the results in Isaac Lab are promising, this validation remains a ``Sim-to-Sim'' evaluation within the same physics engine (PhysX). 
The main risk is that the policy may have learned to exploit simulation-specific artifacts (numerical errors, simplified friction model) that do not exist in reality.
To ensure true robustness before real-world deployment, it is crucial to validate the policy in different simulation environments (e.g., Gazebo, PyBullet with different physics parameters) and ultimately on real hardware.


