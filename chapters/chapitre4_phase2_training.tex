\section{Residual RL Training in Isaac Lab}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{images/train.png}
    \caption{Massive parallelization of 4096 environments in Isaac Lab.}
    \label{fig:isaac_lab_training}
\end{figure}

\section{Synthetic Perception \& Noise}
To bridge the \textit{Sim2Real} gap (Reality Gap), real-world imperfections are explicitly modeled in the simulator. 
These perturbations affect both the agent's perception and the physical dynamics:

\begin{figure}[htbp]
    \centering
    \tikzstyle{block} = [rectangle, draw, fill=blue!10, text width=3.5cm, text centered, rounded corners, minimum height=3.5em]
    \tikzstyle{noise} = [rectangle, draw=red!80, fill=red!10, text width=3.5cm, text centered, dashed, minimum height=3.5em]
    \tikzstyle{physics} = [rectangle, draw=green!80, fill=green!10, text width=4cm, text centered, minimum height=3.5em]
    \tikzstyle{line} = [draw, -latex, thick]
    
    \begin{tikzpicture}[node distance = 2cm and 2.8cm, auto]
        % --- Colonne Centrale : Action Flow ---
        % 1. Agent
        \node [block, fill=gray!20] (agent) {\textbf{Agent (Policy)}};
        
        % 2. Latency (En dessous)
        \node [noise, below=of agent] (latency) {\textbf{Action Latency}};
        
        % 3. Physique (En dessous)
        \node [physics, below=of latency] (sim) {\textbf{Physics Engine}\\ (Isaac Lab)};
        
        % --- Colonne Gauche : Perturbations Physiques ---
        \node [noise, left=of sim] (dynamics) {\textbf{Dynamics Randomization}};
        
        % --- Colonne Droite : Perception ---
        % 4. Camera (A droite du simu)
        \node [block, right=of sim] (camera) {\textbf{Camera Model}};
        
        % 5. Bruit Visuel (Au dessus de la camera)
        \node [noise, above=of camera] (cam_noise) {\textbf{Visual Noise}};
        
        % 6. Target Noise (Déplacé à GAUCHE de l'agent pour éviter la collision à droite)
        \node [noise, left=of agent] (target_noise) {\textbf{Target Noise}};
        
        % --- Liens ---
        % Agent -> Latency -> Sim
        \path [line] (agent) -- (latency);
        \path [line] (latency) -- node[right] {Action $a_t$} (sim);
        
        % Sim -> Camera
        \path [line] (sim) -- node[below] {True State} (camera);
        
        % Camera -> Visual Noise
        \path [line] (camera) -- (cam_noise);
        
        % Visual Noise -> Agent (Retour)
        \path [line] (cam_noise) |- node[above, near start] {Noisy State} (agent);
        
        % Target Noise -> Agent (Connecté par la gauche)
        \path [line] (target_noise) -- node[above] {Noisy Goal} (agent);
        
        % Dynamics -> Sim (Déplacé un peu plus bas ou ajusté car Target est maintenant à gauche)
        \path [line, dashed] (dynamics) -- node[above] {Perturbations} (sim);
        
    \end{tikzpicture}
    \caption{Simulation architecture including domain randomization and noise models.}
    \label{fig:domain_randomization_scheme}
\end{figure}

\subsection{Simulation Environment}

\textbf{GPU parallelization for rapid training.} The choice of Isaac Lab is justified by its capability for massive parallelization on GPU. In our configuration, we instantiate \textbf{4096 simultaneous environments} running in parallel. This allows collecting millions of experience samples in minutes, drastically accelerating training compared to real-time sequential simulation or standard CPU-based approaches. Each environment independently simulates the UR10 robot with randomized parameters, enabling robust policy learning across diverse conditions.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{images/prallele_training.png}
\caption{Isaac Lab training with 100 parallel robot environments}
\label{fig:isaac_parallel}
\end{figure}

\textbf{ROS 2 dataset as reference trajectories.} The dataset of 500 trajectories generated with MoveIt2 (Chapter 3) is loaded into Isaac Lab and serves as the nominal reference behavior. During each training episode, a random trajectory from this dataset is sampled, providing the reference joint positions $q_{ref}(t)$ that the agent will learn to improve through residual corrections. This approach ensures the policy learns to enhance an already safe baseline rather than learning control from scratch.

\subsection{Synthetic Perception \& Reality Gap Modeling}

To bridge the sim-to-real gap, we explicitly model real-world imperfections in the simulator. Rather than rendering camera images (which would be computationally expensive), we use a mathematical camera model with injected noise:

\textbf{Camera model (Pinhole).} A virtual camera is placed with realistic intrinsic parameters ($f_x=491.6, f_y=488.0$) matching the physical webcam specifications. The camera's extrinsic pose (position and orientation relative to the robot base) is calibrated to match the real setup.

\textbf{Measurement noise injection:}
\begin{itemize}
  \item \textbf{Sensor jitter (±2mm):} Gaussian noise is added to the 3D Cartesian position measurements of the end-effector: $\sigma_{xyz} = 2$ mm. This simulates depth estimation uncertainty and measurement quantization inherent to real vision systems.
  
  \item \textbf{Calibration error (±2cm):} A random bias is added to the camera's extrinsic pose, sampled uniformly within $\pm 2$ cm: $b_{ext} \sim \mathcal{U}(-0.02, 0.02)$ m. This bias remains constant during an episode but is resampled between episodes, forcing the policy to be robust to calibration inaccuracies.
  
  \item \textbf{Target uncertainty:} The desired target position $EE_{des}$ observed by the agent is corrupted by Gaussian noise ($\sigma_{target} = 2$ mm), simulating uncertainty in goal localization.
\end{itemize}

\textbf{Additional domain randomization:}
\begin{itemize}
  \item \textbf{Action delay:} Communication latency is simulated by delaying action application by 1 to 3 simulation steps (approximately 17--50 ms).
  
  \item \textbf{Dynamics randomization:} Physical parameters are randomized per episode:
  \begin{itemize}
    \item Payload mass attached to end-effector: $m_{load} \sim \mathcal{U}(0.0, 0.5)$ kg
    \item Joint damping scaled by factor $\in [0.8, 1.2]$
    \item Surface friction coefficients: Static $\mu_s \in [0.6, 1.2]$, Dynamic $\mu_d \in [0.5, 1.0]$
  \end{itemize}
\end{itemize}

This comprehensive noise modeling ensures the learned policy is robust to real-world uncertainties rather than overfitting to perfect simulation conditions.

\subsection{PPO Agent Architecture}

\textbf{Observation space ($S_t$, dimension 40).} The agent observes:
\begin{itemize}
  \item \textbf{Proprioception:} Joint positions $q \in \mathbb{R}^6$, joint velocities $\dot{q} \in \mathbb{R}^6$
  \item \textbf{Reference tracking:} Reference joint positions $q_{ref} \in \mathbb{R}^6$, tracking error $e_q = q - q_{ref} \in \mathbb{R}^6$
  \item \textbf{Task-space information:} Measured end-effector position (noisy) $EE_{meas} \in \mathbb{R}^3$, desired target position $EE_{des} \in \mathbb{R}^3$, Cartesian error $e_{EE} = EE_{meas} - EE_{des} \in \mathbb{R}^3$ and its norm $\|e_{EE}\| \in \mathbb{R}^1$
  \item \textbf{Previous actions:} Last applied residual corrections $a_{t-1} \in \mathbb{R}^6$
\end{itemize}

This observation combines proprioceptive sensing (internal robot state), reference trajectory information, and noisy visual feedback (simulated camera measurements), providing the agent with all information needed to compute corrective actions.

\textbf{Action space ($A_t$, dimension 6).} The neural network predicts \textbf{additive joint velocity corrections} (residuals) in radians per second. These values are normalized between $[-1, 1]$ by the $\tanh$ activation function, then scaled by a safety factor $k_{scale} = 0.05$ rad/s. This limits the agent's authority to $\pm 0.05$ rad/s ($\approx 2.8°$/s) around the reference trajectory, ensuring movement safety and stability. The final control command is:
\begin{equation}
a_{total} = a_{ref}(t) + a_{residual}(t)
\end{equation}
where $a_{ref}(t)$ comes from the MoveIt2 trajectory and $a_{residual}(t)$ is the learned correction.

\subsection{Reward Function}

The reward function is designed to maximize tracking precision while minimizing jerky movements and energy consumption. We use an exponential kernel to provide dense, bounded rewards within $[0, 1]$:

\begin{equation}
r_t = \underbrace{\exp\left(-\frac{\|e_{pos}\|^2}{\sigma_{pos}^2}\right)}_{r_{tracking}}
- \underbrace{\lambda_{v} \|\dot{x}_{ee}\|^2}_{p_{smoothness}}
- \underbrace{\lambda_{a} \|\ddot{x}_{ee}\|^2}_{p_{stability}}
+ \underbrace{r_{bonus}}_{r_{guide}}
\end{equation}

\textbf{Component breakdown:}
\begin{itemize}
    \item \textbf{Precision Term}: Rewards proximity to the cartesian target ($\sigma_{pos} = 0.02$ m).
    \item \textbf{Regularization Penalties}:
    \begin{itemize}
        \item End-effector velocity ($\lambda_{v}=0.03$) to avoid jerky movements.
        \item End-effector acceleration ($\lambda_{a}=0.01$) to reduce vibrations (jitter).
    \end{itemize}
    \item \textbf{Bonus}: A constant bonus ($0.5$) is awarded if the error is below a critical threshold (2 cm), incentivizing the agent to maintain final precision.
\end{itemize}

