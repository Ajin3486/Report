\section{Residual RL Training in Isaac Lab}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=1\textwidth]{images/train.png}
    \caption{Massive parallelization of 4096 environments in Isaac Lab.}
    \label{fig:isaac_lab_training}
\end{figure}

\section{Synthetic Perception \& Noise}
To bridge the \textit{Sim2Real} gap (Reality Gap), real-world imperfections are explicitly modeled in the simulator. 
These perturbations affect both the agent's perception and the physical dynamics:



\subsection{Simulation Environment}

\textbf{GPU parallelization for rapid training.} The choice of Isaac Lab is justified by its capability for massive parallelization on GPU. In our configuration, we instantiate \textbf{4096 simultaneous environments} running in parallel. This allows collecting millions of experience samples in minutes, drastically accelerating training compared to real-time sequential simulation or standard CPU-based approaches. Each environment independently simulates the UR10 robot with randomized parameters, enabling robust policy learning across diverse conditions.



\textbf{ROS 2 dataset as reference trajectories.} The dataset of 500 trajectories generated with MoveIt2 (Chapter 3) is loaded into Isaac Lab and serves as the nominal reference behavior. During each training episode, a random trajectory from this dataset is sampled, providing the reference joint positions $q_{ref}(t)$ that the agent will learn to improve through residual corrections. This approach ensures the policy learns to enhance an already safe baseline rather than learning control from scratch.



\subsection{Synthetic Perception \& Reality Gap Modeling}


\begin{figure}[!htb]
    \centering
    \tikzstyle{block} = [rectangle, draw, fill=blue!10, text width=3.5cm, text centered, rounded corners, minimum height=3.5em]
    \tikzstyle{noise} = [rectangle, draw=red!80, fill=red!10, text width=3.5cm, text centered, dashed, minimum height=3.5em]
    \tikzstyle{physics} = [rectangle, draw=green!80, fill=green!10, text width=4cm, text centered, minimum height=3.5em]
    \tikzstyle{line} = [draw, -latex, thick]
    
    \begin{tikzpicture}[node distance = 2cm and 2.8cm, auto]
        % --- Colonne Centrale : Action Flow ---
        % 1. Agent
        \node [block, fill=gray!20] (agent) {\textbf{Agent (Policy)}};
        
        % 2. Latency (En dessous)
        \node [noise, below=of agent] (latency) {\textbf{Action Latency}};
        
        % 3. Physique (En dessous)
        \node [physics, below=of latency] (sim) {\textbf{Physics Engine}\\ (Isaac Lab)};
        
        % --- Colonne Gauche : Perturbations Physiques ---
        \node [noise, left=of sim] (dynamics) {\textbf{Dynamics Randomization}};
        
        % --- Colonne Droite : Perception ---
        % 4. Camera (A droite du simu)
        \node [block, right=of sim] (camera) {\textbf{Camera Model}};
        
        % 5. Bruit Visuel (Au dessus de la camera)
        \node [noise, above=of camera] (cam_noise) {\textbf{Visual Noise}};
        
        % 6. Target Noise (Déplacé à GAUCHE de l'agent pour éviter la collision à droite)
        \node [noise, left=of agent] (target_noise) {\textbf{Target Noise}};
        
        % --- Liens ---
        % Agent -> Latency -> Sim
        \path [line] (agent) -- (latency);
        \path [line] (latency) -- node[right] {Action $a_t$} (sim);
        
        % Sim -> Camera
        \path [line] (sim) -- node[below] {True State} (camera);
        
        % Camera -> Visual Noise
        \path [line] (camera) -- (cam_noise);
        
        % Visual Noise -> Agent (Retour)
        \path [line] (cam_noise) |- node[above, near start] {Noisy State} (agent);
        
        % Target Noise -> Agent (Connecté par la gauche)
        \path [line] (target_noise) -- node[above] {Noisy Goal} (agent);
        
        % Dynamics -> Sim (Déplacé un peu plus bas ou ajusté car Target est maintenant à gauche)
        \path [line, dashed] (dynamics) -- node[above] {Perturbations} (sim);
        
    \end{tikzpicture}
    \caption{Simulation architecture including domain randomization and noise models.}
    \label{fig:domain_randomization_scheme}
\end{figure}
\vspace{0.5cm}

\textbf{Architecture overview.} Figure \ref{fig:domain_randomization_scheme} illustrates the complete simulation pipeline with all sources of uncertainty. The agent's policy outputs actions that pass through a latency buffer (red dashed box) before reaching the physics engine. The simulated robot state is observed through a noisy camera model (blue) which adds visual jitter, while the target goal is also corrupted by noise (red, left). Additionally, physical parameters like mass, friction, and damping are randomly perturbed (green, left) to force the policy to generalize across different robot dynamics. This multi-layer noise injection ensures the learned policy is robust to the reality gap.

\vspace{0.5cm}

To bridge the sim-to-real gap, we explicitly model real-world imperfections in the simulator. Rather than rendering camera images (which would be computationally expensive), we use a mathematical camera model with injected noise:

\textbf{Camera model (Pinhole).} A virtual camera is placed with realistic intrinsic parameters ($f_x=491.6, f_y=488.0$) matching the physical webcam specifications. The camera's extrinsic pose (position and orientation relative to the robot base) is calibrated to match the real setup.

\textbf{Measurement noise injection:}
\begin{itemize}
  \item \textbf{Sensor jitter (±2mm):} Gaussian noise is added to the 3D Cartesian position measurements of the end-effector: $\sigma_{xyz} = 2$ mm. This simulates depth estimation uncertainty and measurement quantization inherent to real vision systems.
  
  \item \textbf{Calibration error (±2cm):} A random bias is added to the camera's extrinsic pose, sampled uniformly within $\pm 2$ cm: $b_{ext} \sim \mathcal{U}(-0.02, 0.02)$ m. This bias remains constant during an episode but is resampled between episodes, forcing the policy to be robust to calibration inaccuracies.
  
  \item \textbf{Target uncertainty:} The desired target position $EE_{des}$ observed by the agent is corrupted by Gaussian noise ($\sigma_{target} = 2$ mm), simulating uncertainty in goal localization.
\end{itemize}

\textbf{Additional domain randomization:}
\begin{itemize}
  \item \textbf{Action delay:} Communication latency is simulated by delaying action application by 1 to 3 simulation steps (approximately 17--50 ms).
  
  \item \textbf{Dynamics randomization:} Physical parameters are randomized per episode:
  \begin{itemize}
    \item Payload mass attached to end-effector: $m_{load} \sim \mathcal{U}(0.0, 0.5)$ kg
    \item Joint damping scaled by factor $\in [0.8, 1.2]$
    \item Surface friction coefficients: Static $\mu_s \in [0.6, 1.2]$, Dynamic $\mu_d \in [0.5, 1.0]$
  \end{itemize}
\end{itemize}

This comprehensive noise modeling ensures the learned policy is robust to real-world uncertainties rather than overfitting to perfect simulation conditions.

\subsection{Proximal Policy Optimization (PPO)}

We use Proximal Policy Optimization (PPO) \cite{schulman2017proximal}, a state-of-the-art policy gradient algorithm chosen for its stability and sample efficiency in continuous robotic control. PPO constrains policy updates through a clipped objective function that prevents large destructive changes, ensuring monotonic improvement while reusing collected experience through mini-batch optimization.

\textbf{Learning from rewards.} PPO iteratively improves the policy $\pi_\theta$ by maximizing the clipped surrogate objective:
\begin{equation}
L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min\left(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right) \right]
\end{equation}
where $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ is the probability ratio and $\epsilon=0.2$ limits update magnitude. The advantage $\hat{A}_t$ measures how much better an action is compared to average, estimated using Generalized Advantage Estimation (GAE) from our reward signal.

The exponential reward kernel $\exp(-\|e_{pos}\|^2/\sigma^2)$ provides dense, differentiable gradients at every timestep, enabling PPO to learn from incremental improvements. The velocity and acceleration penalties naturally shape smooth corrections, while the bonus term accelerates convergence toward sub-centimeter accuracy. Across 4096 parallel environments, PPO collects diverse experiences and updates the policy to balance precision, smoothness, and energy efficiency.

\subsection{PPO Agent Architecture}

\textbf{Observation space ($S_t$, dimension 40).} The agent observes:
\begin{itemize}
  \item \textbf{Proprioception:} Joint positions $q \in \mathbb{R}^6$, joint velocities $\dot{q} \in \mathbb{R}^6$
  \item \textbf{Reference tracking:} Reference joint positions $q_{ref} \in \mathbb{R}^6$, tracking error $e_q = q - q_{ref} \in \mathbb{R}^6$
  \item \textbf{Task-space information:} Measured end-effector position (noisy) $EE_{meas} \in \mathbb{R}^3$, desired target position $EE_{des} \in \mathbb{R}^3$, Cartesian error $e_{EE} = EE_{meas} - EE_{des} \in \mathbb{R}^3$ and its norm $\|e_{EE}\| \in \mathbb{R}^1$
  \item \textbf{Previous actions:} Last applied residual corrections $a_{t-1} \in \mathbb{R}^6$
\end{itemize}

This observation combines proprioceptive sensing (internal robot state), reference trajectory information, and noisy visual feedback (simulated camera measurements), providing the agent with all information needed to compute corrective actions.

\textbf{Action space ($A_t$, dimension 6).} The neural network predicts \textbf{additive joint velocity corrections} (residuals) in radians per second. These values are normalized between $[-1, 1]$ by the $\tanh$ activation function, then scaled by a safety factor $k_{scale} = 0.05$ rad/s. This limits the agent's authority to $\pm 0.05$ rad/s ($\approx 2.8°$/s) around the reference trajectory, ensuring movement safety and stability. The final control command is:
\begin{equation}
a_{total} = a_{ref}(t) + a_{residual}(t)
\end{equation}
where $a_{ref}(t)$ comes from the MoveIt2 trajectory and $a_{residual}(t)$ is the learned correction.

\subsection{Reward Function}

The reward function is the critical signal that drives PPO's learning process. It defines what constitutes "good" behavior and directly shapes the learned policy. Our reward is designed to maximize tracking precision while minimizing jerky movements and energy consumption. We use an exponential kernel to provide dense, bounded rewards within $[0, 1]$:

\begin{equation}
r_t = \underbrace{\exp\left(-\frac{\|e_{pos}\|^2}{\sigma_{pos}^2}\right)}_{r_{tracking}}
- \underbrace{\lambda_{v} \|\dot{x}_{ee}\|^2}_{p_{smoothness}}
- \underbrace{\lambda_{a} \|\ddot{x}_{ee}\|^2}_{p_{stability}}
+ \underbrace{r_{bonus}}_{r_{guide}}
\end{equation}

\textbf{Component breakdown:}
\begin{itemize}
    \item \textbf{Precision Term}: Rewards proximity to the cartesian target ($\sigma_{pos} = 0.02$ m).
    \item \textbf{Regularization Penalties}:
    \begin{itemize}
        \item End-effector velocity ($\lambda_{v}=0.03$) to avoid jerky movements.
        \item End-effector acceleration ($\lambda_{a}=0.01$) to reduce vibrations (jitter).
    \end{itemize}
    \item \textbf{Bonus}: A constant bonus ($0.5$) is awarded if the error is below a critical threshold (2 cm), incentivizing the agent to maintain final precision.
\end{itemize}

\section{Training Results}
We evaluated the learning process by monitoring the average reward per episode.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{images/learning_curve.png}
    \caption{Learning curves: average reward per episode during training.}
    \label{fig:learning_curve}
\end{figure}

The results show rapid and stable convergence. Figure~\ref{fig:learning_curve} illustrates the training progress, which can be analyzed in three distinct phases:

\begin{enumerate}
    \item \textbf{Exploration Phase (Initial Steps):} In the early stage of training, the agent explores the action space with high entropy. The reward is low and variance is high as the policy has not yet learned to compensate for the systematic errors (calibration and noise).
    
    \item \textbf{Rapid Improvement Phase:} As the PPO algorithm updates the policy, the agent quickly identifies that minimizing the distance to the target yields the highest reward. The curve shows a steep ascent, indicating that the residual corrections are effectively reducing the tracking error.
    
    \item \textbf{Convergence and Fine-tuning:} In the final stage, the curve plateaus. The agent has learned a robust policy and is now fine-tuning its actions to maximize the secondary reward components (smoothness and stability). The stability of this plateau confirms that the policy has converged to an optimal solution without catastrophic forgetting.
\end{enumerate}

This efficient learning process demonstrates the effectiveness of the residual formulation: since the agent only needs to learn small corrections around a valid base trajectory, the problem is much simpler than learning full control from scratch.

\subsubsection{Analysis of individual reward components}
To better understand the agent's behavior, we analyze the evolution of specific reward terms during training (as shown in the subplots of Figure~\ref{fig:learning_curve}):

\begin{itemize}
    \item \textbf{Mean Episode Reward:} The primary metric showing the agent's performance. It starts near zero and rapidly increases, reaching a plateau around reward value of 0.8-0.9. This exponential growth demonstrates that the agent successfully learned to minimize tracking error while respecting smoothness constraints. The stable convergence without oscillations indicates robust learning.
    
    \item \textbf{Mean Episode Length:} Tracks how long episodes run before termination. Initially very short (25-30 steps) due to frequent failures, it quickly increases to the maximum of 64 steps, showing that the agent learned to complete entire trajectories without early termination.
    
    \item \textbf{Policy Loss (policy\_loss):} This metric starts high and decreases, reflecting the optimization of the actor network. The fluctuations indicate the continuous adjustment of the policy to find better actions.
    
    \item \textbf{Value Loss (value\_loss):} The value function estimator (Critic) rapidly reduces its prediction error. A low stable value loss means the Critic accurately predicts the expected return of the current state, which is crucial for stable PPO training.
    
    \item \textbf{Entropy (entropy):} The entropy curve shows a gradual decrease. Initially high, it encourages exploration of random actions. As training progresses, the agent becomes more confident in its optimal strategy, and the distribution of actions narrows (less randomness).
    
    \item \textbf{Explained Variance (explained\_var):} Measures how well the value function predicts returns. Values close to 1.0 indicate excellent predictions, confirming the Critic's accuracy in estimating future rewards.
    
    \item \textbf{Learning Rate (lr):} Shows the adaptive learning rate schedule. It may decrease over time to allow finer policy adjustments as training progresses, ensuring stable convergence in later stages.
\end{itemize}


