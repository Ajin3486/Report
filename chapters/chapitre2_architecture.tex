
This chapter provides an overview of the complete system architecture before diving into implementation details. We first present the global software pipeline connecting all components, then describe the physical hardware setup, and finally explain the coordinate transformation framework that enables precise motion planning.

\section{Global Software Pipeline}

Figure \ref{fig:global_pipeline} presents the complete data flow from the physical setup to validation. This pipeline consists of five key stages, each playing a specific role in the residual reinforcement learning approach.

\begin{figure}[htbp]
    \centering
    \hspace*{-1.2cm}
    % Configuration du style des boîtes
    \begin{tikzpicture}[
        node distance=0.7cm and 1.1cm,
        auto,
        >=latex',
        thick,
        % Style des boîtes principales
        block/.style={
            rectangle, 
            draw=black!55, 
            fill=blue!5, 
            text width=2.5cm, 
            align=center, 
            rounded corners, 
            minimum height=3.5em,
            drop shadow
        },
        % Style des boîtes de titre (étapes)
        steptext/.style={
            font=\bfseries\small, 
            color=blue!60!black, 
            below=0.1cm
        },
        % Style des flèches
        line/.style={
            ->, 
            ultra thick, 
            color=gray!70!black
        }
    ]

    % --- ÉTAPE 1 : MATÉRIEL ---
    \node [block, fill=gray!10] (setup) {Real UR10\\Camera + Tags\\(Hardware)};
    \node [steptext] at (setup.south) {1. Physical Setup};

    % --- ÉTAPE 2 : ROS 2 ---
    \node [block, right=of setup, fill=orange!10] (ros) {\textbf{ROS 2 / MoveIt}\\Trajectory\\Generation ($q_{ref}$)};
    \node [steptext] at (ros.south) {2. Planning};

    % --- ÉTAPE 3 : ISAAC LAB ---
    \node [block, right=of ros, fill=green!10] (isaac) {\textbf{Isaac Lab}\\PPO Training\\};
    \node [steptext] at (isaac.south) {3. Learning};

    % --- ÉTAPE 4 : RÉSIDUEL ---
    \node [block, right=of isaac, fill=purple!10] (residual) {\textbf{Residual Control}\\$a_{tot} = a_{ref} + a_{rl}$\\Correction};
    \node [steptext] at (residual.south) {4. Fusion};

    % --- ÉTAPE 5 : VALIDATION ---
    \node [block, right=of residual, fill=red!10] (valid) {\textbf{Validation}\\Isaac Lab\\};
    \node [steptext] at (valid.south) {5. Evaluation};

    % --- FLÈCHES DE CONNEXION ---
    \draw [line] (setup) -- node[above, font=\tiny, text=black] {Tag poses} (ros);
    \draw [line] (ros) -- node[above, font=\tiny, align=center, text=black] {Dataset\\(.npz)} (isaac);
    \draw [line] (isaac) -- node[above, font=\tiny, text=black] {Policy $\pi$} (residual);
    \draw [line] (residual) -- node[above, font=\tiny, text=black] {Actions} (valid);

    \end{tikzpicture}
    \caption{Global project pipeline from physical setup to validation.}
    \label{fig:global_pipeline}
\end{figure}

\textbf{Stage 1: Physical Setup.} The real UR10 robot equipped with camera and AprilTags provides tag pose measurements that serve as targets for motion planning (detailed in Section 2.2).

\textbf{Stage 2: ROS 2 Planning.} MoveIt2 generates nominal trajectories ($q_{ref}$) between detected tag positions using Cartesian path interpolation. These trajectories serve as reference motions that the RL policy will learn to improve through residual corrections (detailed in Chapter 3).

\textbf{Stage 3: Isaac Lab Learning.} The dataset of reference trajectories is loaded into Isaac Lab where a PPO agent learns residual corrections in massively parallel simulation environments. Domain randomization introduces realistic perturbations to encourage robust policy learning (detailed in Chapter 4).

\textbf{Stage 4: Residual Control Fusion.} The learned policy produces residual actions ($a_{rl}$) that are added to the nominal actions ($a_{ref}$) to obtain the final control: $a_{tot} = a_{ref} + a_{rl}$. This additive structure ensures the policy starts from a safe baseline and only learns corrective adjustments.

\textbf{Stage 5: Validation in Isaac Lab.} After training, we compare the performance of three approaches: (1) MoveIt baseline alone (no RL), (2) RL policy alone (no MoveIt), and (3) Residual control (MoveIt + RL). This comparison demonstrates the benefit of combining classical planning with learned corrections under realistic noise and calibration errors (detailed in Chapter 5).

\section{Hardware Setup}

\begin{figure}[h]
\centering
\begin{overpic}[width=0.35\textwidth]{images/Robot_reel.jpg}
  \put(21,52){\colorbox{white}{\textbf{\textcolor{red}{1}}}}
  \put(26,26){\colorbox{white}{\textbf{\textcolor{blue}{2}}}}
  \put(22,13){\colorbox{white}{\textbf{\textcolor{green!60!black}{3}}}}
  \put(24,93){\colorbox{white}{\textbf{\textcolor{orange}{4}}}}
\end{overpic}
\caption{Real-world setup: (1) UR10 manipulator, (2) reference tag, (3) Workspace area, (4) Camera}
\label{fig:real_setup}
\end{figure}

The physical system (Figure \ref{fig:real_setup}) corresponds to Stage 1 of the pipeline and consists of:
\begin{itemize}
  \item \textbf{UR10 manipulator (1):} 6 degrees of freedom industrial robot for trajectory following tasks.
  \item \textbf{Reference tag (2):} AprilTag marker anchoring the world coordinate system to ensure stable and repeatable measurements across all experiments.
  \item \textbf{Workspace (3):} Defined planar area with dimensions X: [0.7m, 1.0m], Y: [-0.2m, 0.2m], Z: 0.2m fixed height. This constrained workspace simplifies the learning problem while remaining representative of real trajectory following scenarios.
  \item \textbf{Camera (4):} Webcam mounted in an eye-to-hand configuration for AprilTag detection and 3D localization of target objects.
\end{itemize}

\subsection{Kinematic and Dynamic Modeling}

To ensure accurate control and enable seamless sim-to-real transfer, the kinematic and dynamic models of the UR10 are derived from the official Unified Robot Description Format (URDF) provided by Universal Robots via the \texttt{ur\_description} ROS package. The kinematic model defines the physical dimensions, joint limits, and collision meshes matching our physical robot. The dynamic model incorporates the manufacturer-specified inertial properties (mass, center of mass, inertia tensors) and joint friction coefficients. These official specifications are imported identically into both the MoveIt planning scene and the Isaac Lab physics engine to maintain perfect consistency between simulation training and intended real-world deployment.

\subsection{Vision System Calibration}

\textbf{Camera hardware.} We use a Logitech C930e webcam (4) configured at 640$\times$480 resolution at 30 fps, providing sufficient quality for real-time AprilTag detection.

\textbf{AprilTag markers.} Four fiducial markers from the 36h11 family (10cm $\times$ 10cm, IDs 0--3) are used for localization. Tag ID 1, labeled as (2) in Figure \ref{fig:real_setup}, serves as the reference anchor frame.

\textbf{Intrinsic parameters.} Camera intrinsics were calibrated using a checkerboard pattern, yielding focal lengths $f_x=507.71$, $f_y=507.01$, principal point $(c_x, c_y)=(318.97, 237.61)$, and plumb\_bob distortion coefficients $(k_1, k_2, p_1, p_2, k_3) = (0.149, -0.675, -0.002, -0.001, 1.382)$.

\textbf{Extrinsic calibration and coordinate frames.} To enable motion planning in the robot's coordinate system, we must establish a common reference frame between the perception system (camera (4)) and the actuation system (robot (1)). We achieve this by using the reference fiducial marker (tag 1, labeled as (2) in Figure \ref{fig:real_setup}) as a global anchor.

\begin{figure}[htb]
\centering
\includegraphics[width=0.5\textwidth]{images/Ros2_env.jpeg}
\caption{ROS2 TF tree visualization showing the transformation chain from robot base through reference tag and camera to detected targets}
\label{fig:ros_tf}
\end{figure}

This anchor frame is critical: without it, the coordinate system would drift as the camera moves or vibrates, causing inconsistent target positions. Additionally, using a dedicated reference tag ensures continuous visibility of the world frame, whereas task-specific tags may be occluded by the robot arm during manipulation. The transformation between the robot base frame and this reference tag was determined through manual physical measurement. The measured translation vector is:
\begin{equation}
\mathbf{t}_{\text{base}}^{\text{ref}} = [\Delta x, \Delta y, \Delta z]^T = [-0.193, -0.255, -0.01]^T \text{ (m)}
\end{equation}

This vector is broadcast as a static TF transformation in ROS 2, allowing the system to continuously compute the camera's extrinsic pose relative to the robot base by chaining transformations:
\begin{equation}
T_{\text{base}}^{\text{camera}} = T_{\text{base}}^{\text{ref}} \cdot (T_{\text{camera}}^{\text{ref}})^{-1}
\end{equation}

The complete transformation chain (Figure \ref{fig:ros_tf}) enables converting any detected tag position to the robot's base frame for motion planning:
\begin{equation}
T_{\text{base}}^{\text{tag}} = T_{\text{base}}^{\text{ref}} \cdot T_{\text{ref}}^{\text{cam}} \cdot T_{\text{cam}}^{\text{tag}}
\end{equation}
where $T_{\text{cam}}^{\text{tag}}$ is the vision-based detection, $T_{\text{ref}}^{\text{cam}}$ is obtained by inverting the detected reference tag pose, and $T_{\text{base}}^{\text{ref}}$ is the manually measured static offset.


