% Chapter 1: Background (~2.5 pages)

\section{Motion Planning and ROS2/MoveIt}

Motion planning addresses the challenge of computing collision-free trajectories from an initial to a goal configuration. MoveIt, integrated with the Robot Operating System 2 (ROS2), is the de facto standard for motion planning in robotics research and industry. It provides sampling-based planners (e.g., RRT, RRT*) and optimization-based methods (e.g., CHOMP, STOMP) to generate trajectories that respect joint limits, avoid obstacles, and minimize path length or execution time.

However, classical motion planners operate under the assumption that the robot model and environment representation are accurate. When calibration errors exist—such as inaccurate camera extrinsics or AprilTag position uncertainties—MoveIt generates trajectories to the \textit{perceived} target location rather than the true one. The planner has no mechanism to detect or compensate for these systematic errors during execution. This limitation motivates the need for adaptive control strategies that can correct for perception-induced positioning errors in real-time.

\section{Reinforcement Learning for Robotics}

\begin{figure}[h]
\centering
\includegraphics[width=0.3\textwidth]{images/Reinforcement_learning_diagram.png}
\caption{Reinforcement Learning cycle: The agent observes the state, takes an action, receives a reward from the environment, and transitions to a new state.}
\label{fig:rl_cycle}
\end{figure}

Reinforcement Learning (RL) teaches a robot through trial and error, like learning to ride a bicycle. As shown in Figure \ref{fig:rl_cycle}, the robot (agent) observes its state, takes an action, and receives a reward (positive if good, negative if bad). Through thousands of repetitions, it learns which actions lead to success.

\textbf{Concrete example.} Consider the robot trying to reach a target position. At timestep $t$, the state might be: current joint angles $[0.1, -0.5, 1.2, -1.1, -1.6, 0.0]$ rad and target distance $0.15$ m. The policy outputs an action: joint velocity corrections $[0.02, -0.01, 0.03, 0.0, -0.01, 0.0]$ rad/s. If this action reduces the distance to $0.10$ m, the reward is positive ($r_t = +0.5$); if distance increases, the reward is negative. After observing 10,000 such examples, the policy learns which joint motions consistently reduce target distance.

\textbf{The simulation solution.} Training on a physical robot is impractical—it would take weeks and risk damage. Instead, NVIDIA Isaac Lab runs 4096 simulated robots simultaneously on GPU, reducing training from weeks to hours. Once trained in simulation, the policy can be deployed to the real robot (sim-to-real transfer).

\textbf{PPO algorithm.} We use Proximal Policy Optimization (PPO) because it is stable (smooth learning without erratic changes) and sample-efficient (learns without excessive trials). PPO has proven successful in various robotics applications from locomotion to manipulation.

\section{Residual Learning}

Training a robot to control all joints from scratch is challenging and potentially unsafe. Residual learning offers a smarter approach by combining a safe baseline controller with learned corrections. The final action is:
\begin{equation}
a_{\text{final}} = a_{\text{nominal}} + a_{\text{residual}}
\end{equation}

In our system, MoveIt generates the nominal trajectory $a_{\text{nominal}}$ ensuring collision-free motion. The RL policy learns only small corrections $a_{\text{residual}}$ to compensate for perception errors and calibration inaccuracies that MoveIt cannot handle.

This provides three key advantages: (1) \textit{Safety}—MoveIt guarantees feasible trajectories while corrections are bounded, (2) \textit{Faster learning}—the policy only learns corrections, not full control, drastically reducing the search space, and (3) \textit{Interpretability}—failures can be traced to either the planner or the learned component.


