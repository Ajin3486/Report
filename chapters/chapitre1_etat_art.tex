% Chapter 1: Background (~2.5 pages)

\section{Motion Planning and ROS2/MoveIt}

Motion planning addresses the challenge of computing collision-free trajectories from an initial to a goal configuration. MoveIt, integrated with the Robot Operating System 2 (ROS2), is the de facto standard for motion planning in robotics research and industry. It provides sampling-based planners (e.g., RRT, RRT*) and optimization-based methods (e.g., CHOMP, STOMP) to generate trajectories that respect joint limits, avoid obstacles, and minimize path length or execution time.

However, classical motion planners operate under the assumption that the robot model and environment representation are accurate. When calibration errors exist—such as inaccurate camera extrinsics or AprilTag position uncertainties—MoveIt generates trajectories to the \textit{perceived} target location rather than the true one. The planner has no mechanism to detect or compensate for these systematic errors during execution. This limitation motivates the need for adaptive control strategies that can correct for perception-induced positioning errors in real-time.

\section{Reinforcement Learning for Robotics}

\begin{figure}[h]
\centering
\includegraphics[width=0.3\textwidth]{images/Reinforcement_learning_diagram.png}
\caption{The Reinforcement Learning cycle: interaction between the agent (policy) and the environment.}
\label{fig:rl_cycle}
\end{figure}

Reinforcement Learning (RL) is a computational framework for learning optimal control policies through interaction with an environment. Unlike supervised learning, which relies on labeled datasets, RL agents learn by maximizing a cumulative reward signal. As illustrated in Figure \ref{fig:rl_cycle}, the agent observes the state $s_t$, executes an action $a_t$, and receives a scalar reward $r_t$. Through repeated episodes, algorithms like Proximal Policy Optimization (PPO) optimize the policy parameters to maximize the expected return over time.

\textbf{Example scenario.} To illustrate the process in robotic manipulation, consider a task where the robot must reach a target. At timestep $t$:
\begin{itemize}
    \item \textbf{State ($s_t$):} The agent perceives the current joint configuration (e.g., $[0.1, -0.5, \dots]$ rad) and the Cartesian distance to the target (e.g., $0.15$ m).
    \item \textbf{Action ($a_t$):} The policy outputs velocity corrections (e.g., $[0.02, -0.01, \dots]$ rad/s).
    \item \textbf{Reward ($r_t$):} If the action reduces the target distance to $0.10$ m, the agent receives a positive reward ($+0.5$). If the distance increases, it receives a penalty.
\end{itemize}
Over millions of such interactions in simulation, the policy converges to a control strategy that consistently minimizes tracking error.

\textbf{Simulation-based training.} Training directly on physical hardware is impractical due to time constraints and safety risks. Instead, we leverage NVIDIA Isaac Lab to simulate thousands of robots in parallel on a single GPU. This allows collecting years of simulated experience in just hours of wall-clock time. Once trained, the policy is transferred to the real robot (Sim-to-Real transfer).

\textbf{Algorithm choice.} We employ Proximal Policy Optimization (PPO), an on-policy gradient method that balances sample efficiency with training stability. PPO prevents destructive policy updates by clipping the objective function, ensuring monotonic improvement without requiring extensive hyperparameter tuning.

\section{Residual Learning}

Training a robot to control all joints from scratch is challenging and potentially unsafe. Residual learning offers a smarter approach by combining a safe baseline controller with learned corrections. The final action is:
\begin{equation}
a_{\text{final}} = a_{\text{nominal}} + a_{\text{residual}}
\end{equation}

In our system, MoveIt generates the nominal trajectory $a_{\text{nominal}}$ ensuring collision-free motion. The RL policy learns only small corrections $a_{\text{residual}}$ to compensate for perception errors and calibration inaccuracies that MoveIt cannot handle.

This provides three key advantages: (1) \textit{Safety}—MoveIt guarantees feasible trajectories while corrections are bounded, (2) \textit{Faster learning}—the policy only learns corrections, not full control, drastically reducing the search space, and (3) \textit{Interpretability}—failures can be traced to either the planner or the learned component.


